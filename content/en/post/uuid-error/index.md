---
title: "UUID Pitfalls in Spark â†’ Kafka â†’ Postgres Pipelines"
date: 2025-06-07
tags: ["spark", "kafka", "postgres", "uuid", "data-engineering", "streaming"]
categories: ["Data Engineering"]
---

I was building a data pipeline using Kafka and Spark structured streaming. Fully containerized. 

The stack:

- **Kafka** for streaming transaction data  
- **Spark Structured Streaming** for real-time processing and fraud detection  
- **Postgres** as the data warehouse

Everything was smooth. Until one tiny villain showed up:

> UUID fields.

Yes â€” UUIDs. 

Hereâ€™s exactly what happened (so you can avoid the same headache).


## âœ… The Original Design 
I designed the tables in Postgres like this:

```sql
CREATE TABLE fact_transaction (
    transaction_id UUID PRIMARY KEY,
    customer_id UUID REFERENCES dim_customer(customer_id),
    merchant_id UUID REFERENCES dim_merchant(merchant_id),
    ...
);
````

Kafka was happily publishing events with UUIDs serialized as strings (since JSON doesnâ€™t do native UUIDs).

Spark was happily reading them using:

```python
schema = StructType([
    StructField("transaction_id", StringType()),
    StructField("customer_id", StringType()),
    StructField("merchant_id", StringType()),
    ...
])
```

All green lights. Until...



## âŒ The "WHY IS THIS FAILING?!" Moment

As Spark tried to write into Postgres via JDBC, kaboom:

```
ERROR: column "transaction_id" is of type uuid but expression is of type character varying
Hint: You will need to rewrite or cast the expression.
```

But waitâ€¦ I *am* passing valid UUID strings!


## ðŸ”¬ Where It Actually Broke (Spoiler: JDBC Shenanigans)

Hereâ€™s what was really happening under the hood:

* Spark reads UUIDs as strings (`StringType()`)
* JDBC sends them to Postgres as `VARCHAR`
* Postgres (strict little thing) says:

  > â€œI'm not auto-casting VARCHAR to UUID when youâ€™re using prepared statements.â€

In plain SQL, Postgres might cast for you.
In JDBC prepared statements? Nope. Strict mode activated.

And to make things worse: Spark doesnâ€™t even have a native `UUIDType()` â€” you're stuck with `StringType()`.


## ðŸš§ False Solutions I Tried (Don't Do This)

I tried all sorts of hacks:

* **Cast UUIDs again in Spark?** â†’ Made no difference.
* **Play with JDBC options like `stringtype=unspecified`?** â†’ Unreliable.
* **Write ugly casting logic in Spark SQL?** â†’ Works today, breaks tomorrow.

Nothing worked cleanly.


## ðŸ”¨ The Real Fix (Simple and Effective)

Finally, I stopped fighting and embraced reality:

> **Store UUIDs as `TEXT` in Postgres.**

```sql
ALTER TABLE fact_transaction ALTER COLUMN transaction_id TYPE TEXT USING transaction_id::TEXT;
```

I updated both fact and dimension tables to store UUIDs as text.

Why? Because:

* Kafka produces UUIDs as strings âœ…
* Spark reads them as `StringType()` âœ…
* JDBC sends them as `VARCHAR` âœ…
* Postgres accepts `TEXT` âœ…

Everything finally *just worked*.


## ðŸ§  Industry Secret: TEXT is Totally Fine

Hereâ€™s the thing most people wonâ€™t tell you:

* Snowflake, BigQuery, Redshift â€” no native UUID types.
* BI tools? They love strings.
* Debugging? Easier with TEXT.
* Schema evolution? Smoother.

Unless youâ€™re building OLTP-level transactional systems, TEXT for UUIDs is often the more *pragmatic* choice in data pipelines.


## ðŸ”„ The Final Architecture 

* Kafka â†’ Spark Streaming â†’ Postgres
* No more casting hacks
* Fully containerized via Docker Compose
* Automated dimension seeding
* Just `docker compose up` â€” and relax


## ðŸš€ Lessons Learned

* UUID + Spark + JDBC + Postgres = ticking time bomb âš ï¸
* TEXT > UUID for analytical pipelines (most of the time)
* Simplify your type system across the stack
* Optimize for **operational smoothness**, not theoretical purity


At the end of the day:  
**Real-world data engineering = hitting weird walls.**  
**Surviving them = becoming a better engineer.**

