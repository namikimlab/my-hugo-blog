+++
title = "ğŸš€ Building a Fintech Batch ETL Pipeline â€” the Modular Way"
date = 2025-09-12
tags = ["data-engineering", "pyspark", "airflow", "dbt", "aws", "etl", "pipeline", "fintech"]
categories = ["Data Engineering"]
+++

---

{{< social-badges >}}

---

![Data Processing Workflow](workflow.png)
## ğŸ¯ Batch Pipeline for Transaction Data  

Imagine: K-pop demon hunters launches a fintech startup for the fans.  
Now they have to deal with **millions of credit card transactions every day** â€” and they need to make sense of them.  

Those transactions power everything from:  

- **Fraud detection** â†’ catch suspicious transactions fast.  
- **Customer insights** â†’ build RFM segments, calculate lifetime value.  
- **Regulatory reporting** â†’ produce accurate, auditable data for the authorities.  
- **Marketing & product analytics** â†’ serve the right offer to the right customer at the right time.  

Without a reliable pipeline, data ends up **late, messy, duplicated** â€” which means **money lost, compliance risk, and very unhappy customers**.  

## ğŸ› ï¸ The Modular Approach  

I wanted to build a **production-style ETL pipeline** that is reproducible, cloud-ready, and analytics-friendly.  
But I quickly realized this is a *big* project â€” and trying to do everything at once would be a nightmare.  

So I broke it into **modular milestones (M0 â†’ M4):**

1. **M0 â€” Core:** ingest â†’ clean â†’ partition (local only)  
2. **M1 â€” Orchestration:** Airflow DAG with daily schedule + backfill  
3. **M2 â€” Modeling:** dbt staging/facts/marts (DuckDB local)  
4. **M3 â€” Cloud:** swap to AWS (S3 + Redshift)  
5. **M4 â€” Governance:** data quality, lineage, dashboards  

This modular setup keeps the project maintainable and shows clear progress 

## ğŸš¦ M0 â€” Core Data Pipeline (Local)

Using **Faker**, I generated ~5,000 synthetic transactions per day for 11 days (Sept 1â€“11).  
Each row looks resembles a real credit card swipe:  
![Sample raw transaction data](sample_data.png)

```text
transaction_id | customer_id | card_id | merchant_id | txn_ts | amount | currency | mcc | status | is_refund | channel 
````

* **mcc:** Merchant Category Code (4-digit number assigned by card networks)
* **channel:** online, POS, or mobile

I deliberately injected **duplicates** and **late arrivals** to simulate messy reality.

Then, I wrote a **PySpark job** (running in Docker) to:

* Deduplicate by `transaction_id` (latest record wins)
* Late data handled (upserted) and re-partitioned by event date
* Write **partitioned Parquet** files to:
  `silver/transactions/year=YYYY/month=MM/day=DD`

âœ… **Result:**
Clean, partitioned Parquet data with <0.001% duplicate rate â€” ready for analytics.

![Partitioned data](sample_partition.png)

## â° M1 â€” Orchestration with Airflow

Having clean data is nice, but I donâ€™t want to run it manually every day.
Enter **Airflow** (in Docker).

I built a simple DAG and add on dbt 

```text
seed_for_day â†’ spark_clean_for_day â†’ dbt_run
```
![Airflow Dag](airflow_dag.png)

Key features:

* **Daily schedule:** runs at 06:00 AM, guaranteed to finish before 08:00
* **Backfills:** one command to reprocess past dates
* **Retries:** automatic retries for transient errors

âœ… **Result:**
A pipeline that can meet **SLAs** (timely data delivery), with **automatic retries** and **backfills**.


## ğŸ“Š M2 â€” Modeling with dbt (DuckDB Local)

Next, I needed **analytics-friendly tables** so business teams can actually use the data.
This is where **dbt** comes in.

I built a dbt project (pointing to DuckDB locally) with:
![dbt lineage graph](dbt_graph.png)

* **Staging:** `stg_transactions` from Parquet
* **Fact:** `fact_transactions` (clean, approved-only)
* **Marts:**
  * `mart_sales_daily`: daily revenue & counts by category
  * `mart_rfm_customer`: recency, frequency, monetary scoring

I added **dbt tests** (unique keys, not null, accepted values) so data quality is enforced automatically.
![dbt test results](dbt_test.png)

Example mart query (`mart_sales_daily`):

```sql
WITH base AS (
  SELECT
    CAST(strftime(txn_ts, '%Y-%m-%d') AS DATE) AS day,
    mcc,
    channel,
    SUM(CASE WHEN is_refund THEN -amount ELSE amount END) AS gross_sales,
    COUNT(*) AS txn_count
  FROM {{ ref('fact_transactions') }}
  GROUP BY 1,2,3
)
SELECT * FROM base
```
Sample output from mart_sales_daily (grain: day Ã— MCC Ã— channel) â€” designed to keep reporting flexible.

![Sample query result](sample_result.png)

âœ… **Result:**
Business-friendly tables for **daily sales trends** and **RFM cohorts** â€” all reproducible in Docker with quality checks built in.

---

## ğŸ Wrapping Up (for Now)

With **M0â€“M2 complete**, I now have:

- âœ… Clean, deduplicated transaction data  
- âœ… Automated daily ingestion with Airflow + backfills  
- âœ… Business-ready marts (sales trends, RFM scores) with dbt tests  

This alone already feels like a mini production pipeline â€” reproducible, automated, and analytics-friendly.

But thereâ€™s more to come. In the **next post (M3â€“M4)**, Iâ€™ll take this local pipeline to the cloud, swapping DuckDB for **AWS S3 + Redshift**, adding **governance, lineage, and dashboards** â€” everything youâ€™d expect in a real fintech data platform.

ğŸ¶ **Level up to the cloud**

![alt text](aws_cloud.png)

---

{{< social-badges >}}

---